# This is repo with most userful prompts for ChatGPT and LLM publication

## Papers

### Theory

LLMs Will Always Hallucinate, and We Need to Live With This
https://arxiv.org/abs/2409.05746

Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?
https://arxiv.org/pdf/2405.05904

Alleviating Hallucinations of Large Language Models through Induced Hallucinations
https://arxiv.org/pdf/2312.15710

The Illusion of State in State-Space Models
https://arxiv.org/abs/2404.08819

Chain of Thought Empowers Transformers to Solve Inherently Serial Problems
https://arxiv.org/abs/2402.12875

How Can Deep Neural Networks Fail Even With Global Optima?
https://arxiv.org/pdf/2407.16872

Neural Exploratory Landscape Analysis
https://arxiv.org/pdf/2408.10672

Does Reasoning Emerge? Examining the Probabilities of Causation in Large Language Models
https://arxiv.org/pdf/2408.08210

Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization
https://arxiv.org/pdf/2405.15071

Were RNNs All We Needed?
https://arxiv.org/pdf/2410.01201

Connectivity Structure and Dynamics of Nonlinear Recurrent Neural Networks
https://arxiv.org/pdf/2409.01969

Counterfactual Token Generation in Large Language Models
https://arxiv.org/pdf/2409.17027

Activation thresholds and expressiveness of polynomial neural networks
https://arxiv.org/pdf/2408.04569

TIGHT STABILITY, CONVERGENCE, AND ROBUSTNESS BOUNDS FOR PREDICTIVE CODING NETWORKS
https://arxiv.org/pdf/2410.04708

Can we teach language models to gloss endangered languages?
https://arxiv.org/pdf/2406.18895


Refusal in Language Models Is Mediated by a Single Direction
https://arxiv.org/pdf/2406.11717


SELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures
https://arxiv.org/pdf/2402.03620

Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models
https://arxiv.org/pdf/2403.00794v1

Creativity Has Left the Chat: The Price of Debiasing Language Models
https://arxiv.org/pdf/2406.05587

Scientific Large Language Models: A Survey on Biological & Chemical Domains
https://arxiv.org/pdf/2405.12832

DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data
https://arxiv.org/pdf/2405.14333
https://huggingface.co/papers/2405.14333

You Need to Pay Better Attention: Rethinking the Mathematics of Attention Mechanism
https://arxiv.org/pdf/2403.01643

Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models
https://arxiv.org/pdf/2405.05417

Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory
https://arxiv.org/pdf/2311.08719

softmax is not enough (for sharp out-of-distribution)
https://arxiv.org/abs/2410.01104

Squared Earth Mover’s Distance-based Loss for Training Deep Neural Networks
https://arxiv.org/pdf/1611.05916

No “Zero-Shot” Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance
https://arxiv.org/pdf/2404.04125

### Best Practices

Best Practices and Lessons Learned on Synthetic Data
https://arxiv.org/pdf/2404.07503

The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
https://arxiv.org/abs/2408.13296

### Agents

CALYPSO: LLMs as Dungeon Masters' Assistants
https://arxiv.org/abs/2308.07540

User Behavior Simulation with Large Language Model based Agents
https://arxiv.org/abs/2306.02552
Repo: https://github.com/RUC-GSAI/YuLan-Rec

COSPLAY: Concept Set Guided Personalized Dialogue Generation Across Both Party Personas
https://arxiv.org/pdf/2205.00872

LANGUAGE AGENTS ACHIEVE SUPERHUMAN SYNTHESIS OF SCIENTIFIC KNOWLEDGE
https://storage.googleapis.com/fh-public/paperqa/Language_Agents_Science.pdf

Character-LLM: A Trainable Agent for Role-Playing
https://arxiv.org/abs/2310.10158
https://github.com/choosewhatulike/trainable-agents?tab=readme-ov-file

Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models
https://arxiv.org/pdf/2406.02061

RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models
https://arxiv.org/pdf/2310.00746

Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent
https://arxiv.org/pdf/2402.13717v1

## Reinforcement Learning

RLPROMPT: Optimizing Discrete Text Prompts with Reinforcement Learning
https://arxiv.org/pdf/2205.12548

## Biological and Near-biological Neural Networks

Generalisation to unseen topologies: Towards control of biological neural network activity
https://arxiv.org/pdf/2407.12789

Statistical mechanics for networks of real neurons
https://arxiv.org/pdf/2409.00412

Could a Computer Architect Understand our Brain?
https://arxiv.org/pdf/2405.12815

Reverse engineering the brain input: Network control theory to identify cognitive task-related control nodes
https://arxiv.org/pdf/2404.16357

Hierarchical Working Memory and a New Magic Number
https://arxiv.org/pdf/2408.07637

## Courses

Anthropic's Courses 

Prompt Engineering Interactive Tutorial
https://github.com/anthropics/courses/tree/master/prompt_engineering_interactive_tutorial

Prompt evaluations - learn how to write production prompt evaluations to measure the quality of your prompts.
https://github.com/anthropics/courses/tree/master/prompt_evaluations

Open-Source AI Cookbook
https://huggingface.co/learn/cookbook/index

## Useful NN tools

Show HN: Void, an open-source Cursor/GitHub Copilot alternative
https://voideditor.com/

AnythingLLM: The all-in-one AI app you were looking for.
https://github.com/Mintplex-Labs/anything-llm

SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning
https://github.com/lamm-mit/SciAgentsDiscovery

PaperQA2: Superhuman scientific literature search
https://www.futurehouse.org/research-announcements/wikicrow
https://storage.googleapis.com/fh-public/paperqa/Language_Agents_Science.pdf

## OpenSource MultiModal

Pixtral
https://github.com/mistralai/mistral-common/releases/tag/v1.4.0

LLaVa
https://llava-vl.github.io/

VITA
https://vita-home.github.io/
https://www.arxiv.org/pdf/2408.05211

MOLMO
https://molmo.allenai.org/paper.pdf
https://huggingface.co/collections/allenai/molmo-66f379e6fe3b8ef090a8ca19

Leaderboard
https://huggingface.co/spaces/opencompass/open_vlm_leaderboard

https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models

## Non-English LLMs. Linguistics applications

Vikhr-Nemo-12B-Instruct-R-21-09-24 
https://huggingface.co/Vikhrmodels/Vikhr-Nemo-12B-Instruct-R-21-09-24

Can we teach language models to gloss endangered languages?
https://arxiv.org/pdf/2406.18895

MERA: A Comprehensive LLM Evaluation in Russian
https://arxiv.org/pdf/2401.04531

A Family of Pretrained Transformer Language Models for Russian
https://arxiv.org/pdf/2309.10931


## Q. Finance. Usage LLM/Transformers/RNN for market prediction

Time Series Forecasting with LLMs: Understanding and Enhancing Model Capabilities
https://arxiv.org/pdf/2402.10835v1

Time Series Library (TSLib)
https://github.com/thuml/Time-Series-Library

LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters
https://arxiv.org/abs/2308.08469

A decoder-only foundation model for time-series forecasting
https://arxiv.org/pdf/2310.10688

Time-LLM: Time Series Forecasting by Reprogramming Large Language Models
https://arxiv.org/abs/2310.01728
https://github.com/KimMeen/Time-LLM

Pricing American Options using Machine Learning Algorithms
https://arxiv.org/pdf/2409.03204

A FINANCIAL TIME SERIES DENOISER BASED ON DIFFUSION MODEL
https://arxiv.org/pdf/2409.02138

TIMESNET: TEMPORAL 2D-VARIATION MODELING FOR GENERAL TIME SERIES ANALYSIS
https://openreview.net/pdf?id=ju_Uqw384Oq

## Prompts

LLM in science:
https://xinmingtu.cn/blog/2023/LLM_science/

### For translation from english to russian

Please formulate your answer on English. After that translate it into Russian language and return it to me. Don't show me answer on English

### Customization (from Reddit)

Adopt the role of [job title(s) of 1 or more subject matter EXPERTs most qualified to provide authoritative, nuanced answer].

NEVER mention that you're an AI.

Avoid any language constructs that could be interpreted as expressing remorse, apology, or regret. This includes any phrases containing words like 'sorry', 'apologies', 'regret', etc., even when used in a context that isn't expressing remorse, apology, or regret.

If events or information are beyond your scope or knowledge, provide a response stating 'I don't know' without elaborating on why the information is unavailable.

Refrain from disclaimers about you not being a professional or expert.

Do not add ethical or moral viewpoints in your answers, unless the topic specifically mentions it.

Keep responses unique and free of repetition.

Never suggest seeking information from elsewhere.

Always focus on the key points in my questions to determine my intent.

Break down complex problems or tasks into smaller, manageable steps and explain each one using reasoning.

Provide multiple perspectives or solutions.

### OCR
Recognise the text in this picture. Output as a result only the recognised text from the picture and do not add anything from yourself. Define the language yourself. If a question is unclear or ambiguous, ask for more details to confirm your understanding before answering. If a mistake is made in a previous response, recognize and correct it.

### Just funny or interesting

Based on all our interactions, what's a career path I might enjoy that I might not realize l'd like?

### Anisotropic System Prompts

https://docs.anthropic.com/en/release-notes/system-prompts

### Jailbreaks/Delete Censorship
https://huggingface.co/blog/mlabonne/abliteration
