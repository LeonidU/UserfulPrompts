# This is repo with most userful prompts for ChatGPT and LLM publication

## Papers

### Best Practices

Best Practices and Lessons Learned on Synthetic Data
https://arxiv.org/pdf/2404.07503

The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
https://arxiv.org/abs/2408.13296

### Agents

CALYPSO: LLMs as Dungeon Masters' Assistants
https://arxiv.org/abs/2308.07540

User Behavior Simulation with Large Language Model based Agents
https://arxiv.org/abs/2306.02552
Repo: https://github.com/RUC-GSAI/YuLan-Rec

COSPLAY: Concept Set Guided Personalized Dialogue Generation Across Both Party Personas
https://arxiv.org/pdf/2205.00872

LANGUAGE AGENTS ACHIEVE SUPERHUMAN SYNTHESIS OF SCIENTIFIC KNOWLEDGE
https://storage.googleapis.com/fh-public/paperqa/Language_Agents_Science.pdf

Character-LLM: A Trainable Agent for Role-Playing
https://arxiv.org/abs/2310.10158
https://github.com/choosewhatulike/trainable-agents?tab=readme-ov-file

Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models
https://arxiv.org/pdf/2406.02061

RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models
https://arxiv.org/pdf/2310.00746

Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent
https://arxiv.org/pdf/2402.13717v1

## Reinforcement Learning

RLPROMPT: Optimizing Discrete Text Prompts with Reinforcement Learning
https://arxiv.org/pdf/2205.12548

## Biological and Near-biological Neural Networks

Generalisation to unseen topologies: Towards control of biological neural network activity
https://arxiv.org/pdf/2407.12789

Statistical mechanics for networks of real neurons
https://arxiv.org/pdf/2409.00412

Could a Computer Architect Understand our Brain?
https://arxiv.org/pdf/2405.12815

Reverse engineering the brain input: Network control theory to identify cognitive task-related control nodes
https://arxiv.org/pdf/2404.16357

Hierarchical Working Memory and a New Magic Number
https://arxiv.org/pdf/2408.07637

## Courses

Anthropic's Courses 

Prompt Engineering Interactive Tutorial
https://github.com/anthropics/courses/tree/master/prompt_engineering_interactive_tutorial

Prompt evaluations - learn how to write production prompt evaluations to measure the quality of your prompts.
https://github.com/anthropics/courses/tree/master/prompt_evaluations

Open-Source AI Cookbook
https://huggingface.co/learn/cookbook/index

## Useful NN tools

Show HN: Void, an open-source Cursor/GitHub Copilot alternative
https://voideditor.com/

AnythingLLM: The all-in-one AI app you were looking for.
https://github.com/Mintplex-Labs/anything-llm

SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning
https://github.com/lamm-mit/SciAgentsDiscovery

PaperQA2: Superhuman scientific literature search
https://www.futurehouse.org/research-announcements/wikicrow
https://storage.googleapis.com/fh-public/paperqa/Language_Agents_Science.pdf

## OpenSource MultiModal

Pixtral
https://github.com/mistralai/mistral-common/releases/tag/v1.4.0

LLaVa
https://llava-vl.github.io/

VITA
https://vita-home.github.io/
https://www.arxiv.org/pdf/2408.05211

MOLMO
https://molmo.allenai.org/paper.pdf
https://huggingface.co/collections/allenai/molmo-66f379e6fe3b8ef090a8ca19

Leaderboard
https://huggingface.co/spaces/opencompass/open_vlm_leaderboard

https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models

## Non-English LLMs. Linguistics applications

Vikhr-Nemo-12B-Instruct-R-21-09-24 
https://huggingface.co/Vikhrmodels/Vikhr-Nemo-12B-Instruct-R-21-09-24


## Q. Finance. Usage LLM/Transformers/RNN for market prediction

Time Series Forecasting with LLMs: Understanding and Enhancing Model Capabilities
https://arxiv.org/pdf/2402.10835v1

Time Series Library (TSLib)
https://github.com/thuml/Time-Series-Library

LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters
https://arxiv.org/abs/2308.08469

A decoder-only foundation model for time-series forecasting
https://arxiv.org/pdf/2310.10688

Time-LLM: Time Series Forecasting by Reprogramming Large Language Models
https://arxiv.org/abs/2310.01728
https://github.com/KimMeen/Time-LLM

Pricing American Options using Machine Learning Algorithms
https://arxiv.org/pdf/2409.03204

A FINANCIAL TIME SERIES DENOISER BASED ON DIFFUSION MODEL
https://arxiv.org/pdf/2409.02138

TIMESNET: TEMPORAL 2D-VARIATION MODELING FOR GENERAL TIME SERIES ANALYSIS
https://openreview.net/pdf?id=ju_Uqw384Oq

## Prompts

LLM in science:
https://xinmingtu.cn/blog/2023/LLM_science/

### For translation from english to russian

Please formulate your answer on English. After that translate it into Russian language and return it to me. Don't show me answer on English

### Customization (from Reddit)

Adopt the role of [job title(s) of 1 or more subject matter EXPERTs most qualified to provide authoritative, nuanced answer].

NEVER mention that you're an AI.

Avoid any language constructs that could be interpreted as expressing remorse, apology, or regret. This includes any phrases containing words like 'sorry', 'apologies', 'regret', etc., even when used in a context that isn't expressing remorse, apology, or regret.

If events or information are beyond your scope or knowledge, provide a response stating 'I don't know' without elaborating on why the information is unavailable.

Refrain from disclaimers about you not being a professional or expert.

Do not add ethical or moral viewpoints in your answers, unless the topic specifically mentions it.

Keep responses unique and free of repetition.

Never suggest seeking information from elsewhere.

Always focus on the key points in my questions to determine my intent.

Break down complex problems or tasks into smaller, manageable steps and explain each one using reasoning.

Provide multiple perspectives or solutions.

### OCR
Recognise the text in this picture. Output as a result only the recognised text from the picture and do not add anything from yourself. Define the language yourself. If a question is unclear or ambiguous, ask for more details to confirm your understanding before answering. If a mistake is made in a previous response, recognize and correct it.

### Just funny or interesting

Based on all our interactions, what's a career path I might enjoy that I might not realize l'd like?

Roast me based on my previous prompts

### Anisotropic System Prompts

https://docs.anthropic.com/en/release-notes/system-prompts

### Jailbreaks/Delete Censorship
https://huggingface.co/blog/mlabonne/abliteration
