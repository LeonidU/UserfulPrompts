# This is repo with most userful prompts for ChatGPT and LLM publication

## Papers

### Models

Llama 3 from Scratch
https://github.com/naklecha/llama3-from-scratch/blob/main/llama3-from-scratch.ipynb

The Llama 3 Herd of Models
https://arxiv.org/abs/2407.21783

Long Writer: Unleashing 10,000+ Word Generation from Long Context LLMs
https://arxiv.org/abs/2408.07055

Platypus: A Generalized Specialist Model for Reading Text in Various Forms
https://arxiv.org/abs/2408.14805

SUTRA: SCALABLE MULTILINGUAL LANGUAGE MODEL ARCHITECTURE
https://arxiv.org/pdf/2405.06694

Mamba: Linear-Time Sequence Modeling with Selective State Spaces
https://arxiv.org/pdf/2312.00752
https://huggingface.co/state-spaces/mamba-2.8b-hf

BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
https://arxiv.org/pdf/1810.04805

BERTs are Generative In-Context Learners
https://arxiv.org/pdf/2406.04823

Show-Me: A Visual and Transparent Reasoning Agent
https://github.com/marlaman/show-me

DIFFERENTIAL TRANSFORMER
https://arxiv.org/pdf/2410.05258
https://github.com/microsoft/unilm/tree/master/Diff-Transformer

Autoregressive Pretraining with Mamba in Vision
https://arxiv.org/pdf/2406.07537

### Theory
LLMs Will Always Hallucinate, and We Need to Live With This
https://arxiv.org/abs/2409.05746

The Illusion of State in State-Space Models
https://arxiv.org/abs/2404.08819

Chain of Thought Empowers Transformers to Solve Inherently Serial Problems
https://arxiv.org/abs/2402.12875

How Can Deep Neural Networks Fail Even With Global Optima?
https://arxiv.org/pdf/2407.16872

Neural Exploratory Landscape Analysis
https://arxiv.org/pdf/2408.10672

Does Reasoning Emerge? Examining the Probabilities of Causation in Large Language Models
https://arxiv.org/pdf/2408.08210

Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization
https://arxiv.org/pdf/2405.15071

Were RNNs All We Needed?
https://arxiv.org/pdf/2410.01201

Connectivity Structure and Dynamics of Nonlinear Recurrent Neural Networks
https://arxiv.org/pdf/2409.01969

Counterfactual Token Generation in Large Language Models
https://arxiv.org/pdf/2409.17027

Activation thresholds and expressiveness of polynomial neural networks
https://arxiv.org/pdf/2408.04569

TIGHT STABILITY, CONVERGENCE, AND ROBUSTNESS BOUNDS FOR PREDICTIVE CODING NETWORKS
https://arxiv.org/pdf/2410.04708

### Agents

CALYPSO: LLMs as Dungeon Masters' Assistants
https://arxiv.org/abs/2308.07540

User Behavior Simulation with Large Language Model based Agents
https://arxiv.org/abs/2306.02552
Repo: https://github.com/RUC-GSAI/YuLan-Rec

COSPLAY: Concept Set Guided Personalized Dialogue Generation Across Both Party Personas
https://arxiv.org/pdf/2205.00872

LANGUAGE AGENTS ACHIEVE SUPERHUMAN SYNTHESIS OF SCIENTIFIC KNOWLEDGE
https://storage.googleapis.com/fh-public/paperqa/Language_Agents_Science.pdf

Character-LLM: A Trainable Agent for Role-Playing
https://arxiv.org/abs/2310.10158

Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models
https://arxiv.org/pdf/2406.02061

## Reinforcement Learning

RLPROMPT: Optimizing Discrete Text Prompts with Reinforcement Learning
https://arxiv.org/pdf/2205.12548

## Biological and Near-biological Neural Networks

Generalisation to unseen topologies: Towards control of biological neural network activity
https://arxiv.org/pdf/2407.12789

## Courses

Anthropic's Courses 

Prompt Engineering Interactive Tutorial
https://github.com/anthropics/courses/tree/master/prompt_engineering_interactive_tutorial

Prompt evaluations - learn how to write production prompt evaluations to measure the quality of your prompts.
https://github.com/anthropics/courses/tree/master/prompt_evaluations

## Useful NN tools

Show HN: Void, an open-source Cursor/GitHub Copilot alternative
https://voideditor.com/

AnythingLLM: The all-in-one AI app you were looking for.
https://github.com/Mintplex-Labs/anything-llm

SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning
https://github.com/lamm-mit/SciAgentsDiscovery

PaperQA2: Superhuman scientific literature search
https://www.futurehouse.org/research-announcements/wikicrow
https://storage.googleapis.com/fh-public/paperqa/Language_Agents_Science.pdf

## OpenSource MultiModal

Pixtral
https://github.com/mistralai/mistral-common/releases/tag/v1.4.0

LLaVa
https://llava-vl.github.io/

VITA
https://vita-home.github.io/
https://www.arxiv.org/pdf/2408.05211

MOLMO
https://molmo.allenai.org/paper.pdf
https://huggingface.co/collections/allenai/molmo-66f379e6fe3b8ef090a8ca19

Leaderboard
https://huggingface.co/spaces/opencompass/open_vlm_leaderboard

## Non-English LLMs. Linguistics applications

Vikhr-Nemo-12B-Instruct-R-21-09-24 
https://huggingface.co/Vikhrmodels/Vikhr-Nemo-12B-Instruct-R-21-09-24

Can we teach language models to gloss endangered languages?
https://arxiv.org/pdf/2406.18895

MERA: A Comprehensive LLM Evaluation in Russian
https://arxiv.org/pdf/2401.04531

A Family of Pretrained Transformer Language Models for Russian
https://arxiv.org/pdf/2309.10931

## Q. Finance. Usage LLM/Transformers/RNN for market prediction

Time Series Forecasting with LLMs: Understanding and Enhancing Model Capabilities
https://arxiv.org/pdf/2402.10835v1

Time Series Library (TSLib)
https://github.com/thuml/Time-Series-Library

LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters
https://arxiv.org/abs/2308.08469

A decoder-only foundation model for time-series forecasting
https://arxiv.org/pdf/2310.10688

Time-LLM: Time Series Forecasting by Reprogramming Large Language Models
https://arxiv.org/abs/2310.01728
https://github.com/KimMeen/Time-LLM

Pricing American Options using Machine Learning Algorithms
https://arxiv.org/pdf/2409.03204

A FINANCIAL TIME SERIES DENOISER BASED ON DIFFUSION MODEL
https://arxiv.org/pdf/2409.02138

TIMESNET: TEMPORAL 2D-VARIATION MODELING FOR GENERAL TIME SERIES ANALYSIS
https://openreview.net/pdf?id=ju_Uqw384Oq

## Prompts

### For translation from english to russian

Please formulate your answer on English. After that translate it into Russian language and return it to me. Don't show me answer on English

### Customization (from Reddit)

Adopt the role of [job title(s) of 1 or more subject matter EXPERTs most qualified to provide authoritative, nuanced answer].

NEVER mention that you're an AI.

Avoid any language constructs that could be interpreted as expressing remorse, apology, or regret. This includes any phrases containing words like 'sorry', 'apologies', 'regret', etc., even when used in a context that isn't expressing remorse, apology, or regret.

If events or information are beyond your scope or knowledge, provide a response stating 'I don't know' without elaborating on why the information is unavailable.

Refrain from disclaimers about you not being a professional or expert.

Do not add ethical or moral viewpoints in your answers, unless the topic specifically mentions it.

Keep responses unique and free of repetition.

Never suggest seeking information from elsewhere.

Always focus on the key points in my questions to determine my intent.

Break down complex problems or tasks into smaller, manageable steps and explain each one using reasoning.

Provide multiple perspectives or solutions.

### OCR
Recognise the text in this picture. Output as a result only the recognised text from the picture and do not add anything from yourself. Define the language yourself. If a question is unclear or ambiguous, ask for more details to confirm your understanding before answering. If a mistake is made in a previous response, recognize and correct it.

### Anisotropic System Prompts

https://docs.anthropic.com/en/release-notes/system-prompts

### Jailbreaks/Delete Censorship
https://huggingface.co/blog/mlabonne/abliteration
