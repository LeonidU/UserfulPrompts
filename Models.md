### Models

Llama 3 from Scratch
https://github.com/naklecha/llama3-from-scratch/blob/main/llama3-from-scratch.ipynb

The Llama 3 Herd of Models
https://arxiv.org/abs/2407.21783

Long Writer: Unleashing 10,000+ Word Generation from Long Context LLMs
https://arxiv.org/abs/2408.07055

Platypus: A Generalized Specialist Model for Reading Text in Various Forms
https://arxiv.org/abs/2408.14805

SUTRA: SCALABLE MULTILINGUAL LANGUAGE MODEL ARCHITECTURE
https://arxiv.org/pdf/2405.06694

Mamba: Linear-Time Sequence Modeling with Selective State Spaces
https://arxiv.org/pdf/2312.00752
https://huggingface.co/state-spaces/mamba-2.8b-hf

BERTs are Generative In-Context Learners
https://arxiv.org/pdf/2406.04823

Show-Me: A Visual and Transparent Reasoning Agent
https://github.com/marlaman/show-me

DIFFERENTIAL TRANSFORMER
https://arxiv.org/pdf/2410.05258
https://github.com/microsoft/unilm/tree/master/Diff-Transformer

Autoregressive Pretraining with Mamba in Vision
https://arxiv.org/pdf/2406.07537

MINT-1T: Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens
https://github.com/mlfoundations/MINT-1T
https://arxiv.org/abs/2406.11271

Generative Reduced Basis Method
https://arxiv.org/pdf/2410.05139

Llama-3.1-Nemotron-70B
https://huggingface.co/collections/nvidia/llama-31-nemotron-70b-670e93cd366feea16abc13d8

FATLLAMA-1.7T-Instruct (1.7 trillion of parameters!!!)
https://huggingface.co/RichardErkhov/FATLLAMA-1.7T-Instruct

TOKENFORMER: RETHINKING TRANSFORMER SCALING WITH TOKENIZED MODEL PARAMETERS
https://arxiv.org/pdf/2410.23168

Understanding the Natural Language of DNA using Encoder-Decoder Foundation Models with Byte-level Precision
https://arxiv.org/pdf/2311.02333v2
